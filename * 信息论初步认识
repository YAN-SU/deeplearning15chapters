五、信息与压缩

很显然，不均匀分布时，某个词出现的概率越高，编码长度就会越短。
从信息的角度看，如果信息内容存在大量冗余，重复内容越多，可以压缩的余地就越大。日常生活的经验也是如此，一篇文章翻来覆去都是讲同样的内容，摘要就会很短。反倒是，每句话意思都不一样的文章，很难提炼出摘要。
图片也是如此，单调的图片有好的压缩效果，细节丰富的图片很难压缩。
由于信息量的多少与概率分布相关，所以在信息论里面，信息被定义成不确定性的相关概念：概率分布越分散，不确定性越高，信息量越大；反之，信息量越小。


六、信息熵

前面公式里的H（平均编码长度），其实就是信息量的度量。H越大，表示需要的二进制位越多，即可能发生的结果越多，不确定性越高。
比如，H为1，表示只需要一个二进制位，就能表示所有可能性，那就只可能有两种结果。如果H为6，六个二进制位表示有64种可能性，不确定性大大提高。
信息论借鉴了物理学，将H称为"信息熵"（information entropy）。在物理学里，熵表示无序，越无序的状态，熵越高。


七、信息量的实例

最后，来看一个例子。如果一个人的词汇量为10万，意味着每个词有10万种可能，均匀分布时，每个词需要 16.61 个二进制位。
log₂(100, 000) = 16.61
所以，一篇1000个词的文章，需要 1.6 万个二进制位（约为 2KB）。
16.61 x 1000 = 16,610
相比之下，一张 480 x 640、16级灰度的图片，需要123万个二进制位（约为 150KB）。
480 x 640 x log₂(16) = 1,228,800
所以，一幅图片所能传递的信息远远超过文字，这就是"一图胜千言"吧。
上面的例子是均匀分布的情况，现实生活中，一般都是不均匀分布，因此文章或图片的实际文件大小都是可以大大压缩的。
